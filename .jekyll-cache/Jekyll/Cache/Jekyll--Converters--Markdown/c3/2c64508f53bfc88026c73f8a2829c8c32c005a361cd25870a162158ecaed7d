I"ñ⁄<h1 id="fastbook---chapter-5---image-classification">Fastbook - Chapter 5 - Image Classification</h1>

<h2 id="about-fastbook">About Fastbook</h2>

<p>Fastbook is a book is focused on the practical side of deep learning. It starts with the big picture, such as definitions and general applications of deep learning, and progressively digs beneath the surface into concrete examples.</p>

<p>The book is based on <em>fastai</em> API, an API on top of Pytorch that makes it easier to use state-of-the-art methods in deep learning.</p>

<p>The early chapters contain the basics of deep learning, so I skimmed them and started to make notes on the fifth chapter. It doesn‚Äôt need you to understand models such as Convolutional Neural Networks and how they work, but it definitely helped me following the book.</p>

<p>I used Google Colab notebooks as it provides free GPU. The downside is that it doesn‚Äôt have any memory available so you will have to install fastai every time you run a notebook. The fastbook package includes fastai and several easy-access datasets to test the models.</p>

<h2 id="installing-fastai-in-google-colab">Installing fastai in Google Colab</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">Uqq</span> <span class="n">fastbook</span>

     <span class="o">|</span><span class="err">‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</span><span class="o">|</span> <span class="mi">727</span><span class="n">kB</span> <span class="mf">29.0</span><span class="n">MB</span><span class="o">/</span><span class="n">s</span>
     <span class="o">|</span><span class="err">‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</span><span class="o">|</span> <span class="mf">1.2</span><span class="n">MB</span> <span class="mf">45.6</span><span class="n">MB</span><span class="o">/</span><span class="n">s</span>
     <span class="o">|</span><span class="err">‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</span><span class="o">|</span> <span class="mi">194</span><span class="n">kB</span> <span class="mf">47.3</span><span class="n">MB</span><span class="o">/</span><span class="n">s</span>
     <span class="o">|</span><span class="err">‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</span><span class="o">|</span> <span class="mi">51</span><span class="n">kB</span> <span class="mf">7.9</span><span class="n">MB</span><span class="o">/</span><span class="n">s</span>
     <span class="o">|</span><span class="err">‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</span><span class="o">|</span> <span class="mi">61</span><span class="n">kB</span> <span class="mf">9.2</span><span class="n">MB</span><span class="o">/</span><span class="n">s</span>
     <span class="o">|</span><span class="err">‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</span><span class="o">|</span> <span class="mi">61</span><span class="n">kB</span> <span class="mf">9.0</span><span class="n">MB</span><span class="o">/</span><span class="n">s</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">fastbook</span>
<span class="n">fastbook</span><span class="p">.</span><span class="n">setup_book</span><span class="p">()</span>
<span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">fastai.vision</span> <span class="kn">import</span> <span class="o">*</span>
</code></pre></div></div>

<h2 id="fastbook-notes---chapter-5-image-classification">Fastbook Notes - Chapter 5: Image Classification</h2>

<p>This chapter focused on building an image classification model.</p>

<h2 id="oxford-iiit-pet-dataset">Oxford-IIIT Pet Dataset</h2>

<p>We will use a images dataset with 37 pet breeds classes and roughly 200 images for each class. The images have large variations in scale, pose, and lighting (here the <a href="https://www.robots.ox.ac.uk/~vgg/data/pets/">original source</a>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Downloading the Oxford-IIIT Pet Dataset
</span><span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="p">.</span><span class="n">PETS</span><span class="p">)</span>
</code></pre></div></div>

<p>We can use the <code class="language-plaintext highlighter-rouge">ls</code> method from fastai to see what is in our dataset and folders</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Path</span><span class="p">.</span><span class="n">BASE_PATH</span> <span class="o">=</span> <span class="n">path</span>
<span class="k">print</span><span class="p">(</span><span class="n">path</span><span class="p">.</span><span class="n">ls</span><span class="p">())</span>
<span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s">"images"</span><span class="p">).</span><span class="n">ls</span><span class="p">()</span>


    <span class="p">[</span><span class="n">Path</span><span class="p">(</span><span class="s">'annotations'</span><span class="p">),</span> <span class="n">Path</span><span class="p">(</span><span class="s">'images'</span><span class="p">)]</span>

    <span class="p">(</span><span class="c1">#7393) [Path('images/newfoundland_31.jpg'),Path('images/Ragdoll_79.jpg'),Path('images/yorkshire_terrier_31.jpg'),Path('images/havanese_172.jpg'),Path('images/newfoundland_61.jpg'),Path('images/Abyssinian_175.jpg'),Path('images/leonberger_164.jpg'),Path('images/saint_bernard_86.jpg'),Path('images/boxer_108.jpg'),Path('images/scottish_terrier_195.jpg')...]
</span></code></pre></div></div>

<h2 id="datablocks">DataBlocks</h2>

<p>Fastai uses DataBlocks to load the data. Here we load the images of the folder into this <code class="language-plaintext highlighter-rouge">DataBlock</code>. Most of the arguments of the functions are quite intuitive to guess, but they are explained below in any case.</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">DataBlock</code> is the envelope of the structure of the data. Here you tell fastai API how you organized the data.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">blocks</code> is how you tell fastai what inputs are images (<code class="language-plaintext highlighter-rouge">ImageBlock</code>) and what are the targets for the categories (<code class="language-plaintext highlighter-rouge">CategoryBlock</code>).</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">get_items</code> is how you tell fastai to assemble our items inside the <code class="language-plaintext highlighter-rouge">DataBlock</code>.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">splitter</code> is used to divide the images in training and validation set randomly.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">get_y</code> is used to create target values. The images are not labeled, they are just 7393 jpgs. We extract the target label (y) from the name of the file using regex expressions <code class="language-plaintext highlighter-rouge">RegexLabeller</code>.</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pets</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">ImageBlock</span><span class="p">,</span> <span class="n">CategoryBlock</span><span class="p">),</span>
                 <span class="n">get_items</span><span class="o">=</span><span class="n">get_image_files</span><span class="p">,</span>
                 <span class="n">splitter</span><span class="o">=</span><span class="n">RandomSplitter</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span>
                 <span class="n">get_y</span><span class="o">=</span><span class="n">using_attr</span><span class="p">(</span><span class="n">RegexLabeller</span><span class="p">(</span><span class="sa">r</span><span class="s">'(.+)_\d+.jpg$'</span><span class="p">),</span> <span class="s">'name'</span><span class="p">),</span>
                 <span class="n">item_tfms</span><span class="o">=</span><span class="n">Resize</span><span class="p">(</span><span class="mi">460</span><span class="p">),</span>
                 <span class="n">batch_tfms</span><span class="o">=</span><span class="n">aug_transforms</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">min_scale</span><span class="o">=</span><span class="mf">0.75</span><span class="p">))</span>

<span class="c1"># Tell DataBlock where the "source" is
</span><span class="n">dls</span> <span class="o">=</span> <span class="n">pets</span><span class="p">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s">"images"</span><span class="p">)</span>
</code></pre></div></div>
<p>We can take the first image and print the path using <code class="language-plaintext highlighter-rouge">.ls()</code> method as well.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fname</span> <span class="o">=</span> <span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s">"images"</span><span class="p">).</span><span class="n">ls</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">fname</span>


    <span class="o">/</span><span class="n">root</span><span class="o">/</span><span class="p">.</span><span class="n">fastai</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">oxford</span><span class="o">-</span><span class="n">iiit</span><span class="o">-</span><span class="n">pet</span><span class="o">/</span><span class="n">images</span><span class="o">/</span><span class="n">newfoundland_31</span><span class="p">.</span><span class="n">jpg</span>
</code></pre></div></div>

<p>Using regex we can extract the target from the jpg name</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">re</span><span class="p">.</span><span class="n">findall</span><span class="p">(</span><span class="sa">r</span><span class="s">'(.+)_\d+.jpg$'</span><span class="p">,</span> <span class="n">fname</span><span class="p">.</span><span class="n">name</span><span class="p">)</span>

          <span class="p">[</span><span class="s">'newfoundland'</span><span class="p">]</span>
</code></pre></div></div>

<p>The last 2 methods are about data augmentation strategy, what fastai call <em>presizing</em>.</p>

<p><strong>Presizing</strong> is a particular way to do image augmentation that is designed to speed up computation and improve model accuracy.</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">item_tfms</code> resize so all the images have the same dimension (In this case 460x460). It is needed so they can collate into tensors to be passed to the GPU. By default, it crops the image (not squish like when you set the background of your computer screen). On the training set, the crop area is chosen randomly. On the validation set, the center square of the image is always chosen.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">batch_tfms</code> randomly random crops and augment parts of the images. It‚Äôs only done once, in one batch. On the validation set, it only resizes to the final size needed for the model. On the training set, it first random crops and performs any augmentations, and then it resizes.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">aug_transforms</code> function can be used to create a list of images flipped, rotated, zoomed, wrapped, or with changed lighting. It helps the training process and avoids overfitting. <code class="language-plaintext highlighter-rouge">min_scale</code> determines how much of the image to select at minimum each time (More <a href="https://docs.fast.ai/vision.augment.html#aug_transforms">here</a>)</p>
  </li>
</ul>

<p><img src="https://raw.githubusercontent.com/fastai/fastbook/780b76bef3127ce5b64f8230fce60e915a7e0735/images/att_00060.png" alt="" /></p>

<h2 id="other-resizing-methods">Other resizing methods</h2>

<p>With <code class="language-plaintext highlighter-rouge">show_batch</code> we can print a batch of images of the training set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Show some images
</span><span class="n">dls</span><span class="p">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/images/Fastbook/Fastbook_Chapter_5_Image_Classification_12_0.png" alt="png" /></p>

<p>We can squish the images, or add padding to the sides or crop it by copying the model with <code class="language-plaintext highlighter-rouge">.new</code> method and modifying the part of the model that you want to change.</p>

<p>Here we squish the images:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pets</span> <span class="o">=</span> <span class="n">pets</span><span class="p">.</span><span class="n">new</span><span class="p">(</span><span class="n">item_tfms</span><span class="o">=</span> <span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">ResizeMethod</span><span class="p">.</span><span class="n">Squish</span><span class="p">))</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">pets</span><span class="p">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s">"images"</span><span class="p">)</span>
<span class="n">dls</span><span class="p">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/images/Fastbook/Fastbook_Chapter_5_Image_Classification_14_0.png" alt="png" /></p>

<p>Here we add padding to the images:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pets</span> <span class="o">=</span> <span class="n">pets</span><span class="p">.</span><span class="n">new</span><span class="p">(</span><span class="n">item_tfms</span><span class="o">=</span> <span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">ResizeMethod</span><span class="p">.</span><span class="n">Pad</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s">'zeros'</span><span class="p">))</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">pets</span><span class="p">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s">"images"</span><span class="p">)</span>
<span class="n">dls</span><span class="p">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/images/Fastbook/Fastbook_Chapter_5_Image_Classification_15_0.png" alt="png" /></p>

<p><strong>Remember that by cropping the images we removed some of the features that allow us to perform recognition.</strong></p>

<p>Instead, what we normally do in practice is to randomly select part of the image and crop it. On each epoch (which is one complete pass through all of our images in the dataset) we randomly select a different crop of each image. We can use <code class="language-plaintext highlighter-rouge">RandomResizedCrop</code> for that.</p>

<p>This means that our model can learn to focus on, and recognize, different features in our images at different epochs. It also reflects how images work in the real world as different photos of the same thing may be framed in slightly different ways.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pets</span> <span class="o">=</span> <span class="n">pets</span><span class="p">.</span><span class="n">new</span><span class="p">(</span><span class="n">item_tfms</span><span class="o">=</span> <span class="n">RandomResizedCrop</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">min_scale</span><span class="o">=</span><span class="mf">0.3</span><span class="p">))</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">pets</span><span class="p">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s">"images"</span><span class="p">)</span>
<span class="c1"># Unique=True to have the same image repeated with different versions of this RandomResizedCrop transform
</span><span class="n">dls</span><span class="p">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">unique</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/images/Fastbook/Fastbook_Chapter_5_Image_Classification_17_0.png" alt="png" /></p>

<p>We can alwasy use <code class="language-plaintext highlighter-rouge">new</code> method to get back to the first resizing method chosen (<code class="language-plaintext highlighter-rouge">aug_transforms</code>):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pets</span> <span class="o">=</span> <span class="n">pets</span><span class="p">.</span><span class="n">new</span><span class="p">(</span><span class="n">item_tfms</span><span class="o">=</span><span class="n">Resize</span><span class="p">(</span><span class="mi">460</span><span class="p">),</span> <span class="n">batch_tfms</span><span class="o">=</span><span class="n">aug_transforms</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">min_scale</span><span class="o">=</span><span class="mf">0.75</span><span class="p">))</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">pets</span><span class="p">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s">"images"</span><span class="p">)</span>
<span class="n">dls</span><span class="p">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">unique</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/images/Fastbook/Fastbook_Chapter_5_Image_Classification_19_0.png" alt="png" /></p>

<h2 id="creating-a-baseline-model">Creating a baseline model</h2>

<p>We can see the shape of the data by printing one batch. Here we printed the labels <code class="language-plaintext highlighter-rouge">y</code>. There are 64 listed numbers printed as the batch size is 64. The range of the numbers goes from 0 to 36 as it represents the labels for the 37 pet breeds.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">dls</span><span class="p">.</span><span class="n">one_batch</span><span class="p">()</span>
<span class="n">y</span>

    <span class="n">TensorCategory</span><span class="p">([</span> <span class="mi">9</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>
    <span class="mi">13</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">29</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span>
    <span class="mi">23</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span>  <span class="mi">7</span><span class="p">]</span>
    <span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">'cuda:0'</span><span class="p">)</span>
</code></pre></div></div>

<p>Training a powerful baseline model requires 2 lines of code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span> <span class="o">=</span> <span class="n">cnn_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet34</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span> <span class="n">error_rate</span><span class="p">)</span>
<span class="n">learn</span><span class="p">.</span><span class="n">fine_tune</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">dls</code> is our data.</li>
  <li><code class="language-plaintext highlighter-rouge">restnet34</code> is a certain pre-trained CNN architecture.</li>
  <li>The <code class="language-plaintext highlighter-rouge">metric</code> requested is <code class="language-plaintext highlighter-rouge">error_rate</code>.</li>
  <li>By default, fast ai chooses the loss function that best fit our kind of data. With image data and a categorical outcome, fastai will default to using <code class="language-plaintext highlighter-rouge">cross-entropy loss</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">fine_tune(2)</code> indicates the number of epochs with the default model configuration.</li>
</ul>

<p>This is the magic and simplicity of fastai. Once you have the data correctly loaded, the modeling with pre-trained models cannot be easier. Fastai automatically download the pre-trained architecture, choses an appropiate loss funtion and prints the metric results:</p>

<table style="center">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.532038</td>
      <td>0.331124</td>
      <td>0.112991</td>
      <td>01:07</td>
    </tr>
  </tbody>
</table>

<table border="0" class="dataframe">
  <thead>
    <tr style="text-align: center; overflow-x:auto;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.514930</td>
      <td>0.295484</td>
      <td>0.094046</td>
      <td>01:12</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.330700</td>
      <td>0.223524</td>
      <td>0.071042</td>
      <td>01:12</td>
    </tr>
  </tbody>
</table>

<p>The second column show the cross-entropy loss in the training and validation set. The fourth column show less than 1% of error classifying the images.</p>

<p>It even includes handy shortcuts like <code class="language-plaintext highlighter-rouge">show_results</code> to print the real and predicted labels for a quick check test of labels and predictions:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span><span class="p">.</span><span class="n">show_results</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/images/Fastbook/Fastbook_Chapter_5_Image_Classification_26_1.png" alt="png" /></p>

<h2 id="model-interpretation">Model interpretation</h2>

<p>After building a model, you don‚Äôt want to know only how many targets got right. You might want to know which targets are harder to predict or which images got wrong to train it better. fastai includes a <code class="language-plaintext highlighter-rouge">ClassificationInterpretation</code> class from which you can call <code class="language-plaintext highlighter-rouge">plot_confusion_matrix</code>,  <code class="language-plaintext highlighter-rouge">most_confused</code> or <code class="language-plaintext highlighter-rouge">plot_top_losses</code> methods to extract this information easily.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">interp</span> <span class="o">=</span> <span class="n">ClassificationInterpretation</span><span class="p">.</span><span class="n">from_learner</span><span class="p">(</span><span class="n">learn</span><span class="p">)</span>
<span class="n">interp</span><span class="p">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/images/Fastbook/Fastbook_Chapter_5_Image_Classification_28_1.png" alt="png" /></p>

<p>We can see which are the labels that the model more struggles to differentiate:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">interp</span><span class="p">.</span><span class="n">most_confused</span><span class="p">(</span><span class="n">min_val</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>

    <span class="p">[(</span><span class="s">'american_pit_bull_terrier'</span><span class="p">,</span> <span class="s">'american_bulldog'</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span>
     <span class="p">(</span><span class="s">'British_Shorthair'</span><span class="p">,</span> <span class="s">'Russian_Blue'</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
     <span class="p">(</span><span class="s">'Ragdoll'</span><span class="p">,</span> <span class="s">'Birman'</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
     <span class="p">(</span><span class="s">'Siamese'</span><span class="p">,</span> <span class="s">'Birman'</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
     <span class="p">(</span><span class="s">'american_pit_bull_terrier'</span><span class="p">,</span> <span class="s">'staffordshire_bull_terrier'</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
     <span class="p">(</span><span class="s">'chihuahua'</span><span class="p">,</span> <span class="s">'miniature_pinscher'</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
     <span class="p">(</span><span class="s">'staffordshire_bull_terrier'</span><span class="p">,</span> <span class="s">'american_pit_bull_terrier'</span><span class="p">,</span> <span class="mi">4</span><span class="p">)]</span>
</code></pre></div></div>

<p>And the the most ‚Äúwrong‚Äù predictions:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">interp</span><span class="p">.</span><span class="n">plot_top_losses</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">nrows</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/images/Fastbook/Fastbook_Chapter_5_Image_Classification_31_0.png" alt="png" /></p>

<h2 id="exporting-and-importing-a-model">Exporting and importing a model</h2>

<p>Models with multiple layers, epochs, and parameters can take hours to train. Instead of starting over every time you run the notebook, the model can be saved and loaded again.</p>

<p><strong>Saving/Exporting a model</strong>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span><span class="p">.</span><span class="n">export</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">abspath</span><span class="p">(</span><span class="s">'./my_export.pkl'</span><span class="p">))</span>
</code></pre></div></div>

<p>To check that the model is saved, you can either navigate the folder and see the <code class="language-plaintext highlighter-rouge">.pkl</code>, or also you can call the <code class="language-plaintext highlighter-rouge">path.ls()</code> method and see the file printed.</p>

<p><strong>Loading/Importing a model</strong>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn_inf</span> <span class="o">=</span> <span class="n">load_learner</span><span class="p">(</span><span class="s">'my_export.pkl'</span><span class="p">)</span>
</code></pre></div></div>
<h2 id="testing-the-model-outside-the-fastai-environment">Testing the model outside the fastai environment</h2>

<p>To see if the model would work outside the dataloader environment, I googled ‚ÄúBengal cat‚Äù in google and drag a random image into the Google Colab folder. I consider the image as tricky, as it contains a human holding a Bengal cat:</p>

<p><img src="/images/Fastbook/test_image.jpg" alt="Random image finded in the internet" width="300" height="400" /></p>

<p>I simply called the <code class="language-plaintext highlighter-rouge">predict</code> method of the model trained before to see if it is as easy at it looks to use fastai.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn_inf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="s">'test_image.jpg'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>('Bengal',
 tensor(1),
 tensor([9.6365e-07, 9.9558e-01, 5.0118e-09, 2.5665e-08, 5.0663e-08, 4.2385e-03, 1.6677e-04, 1.0780e-08, 3.7194e-08, 1.1227e-07, 7.4500e-09, 3.3078e-06, 4.6680e-08, 8.1986e-07, 1.0533e-07, 8.3078e-08,
         9.4154e-08, 2.7704e-08, 2.7787e-07, 2.6699e-06, 2.5465e-06, 7.7660e-09, 8.5412e-09, 1.5087e-07, 3.9640e-08, 3.1239e-08, 9.4404e-07, 3.2094e-08, 5.2541e-08, 7.1558e-09, 4.6352e-09, 1.7388e-08,
         6.1503e-08, 6.6123e-08, 7.2059e-09, 9.4673e-08, 5.6627e-07]))
</code></pre></div></div>

<p>Surprisingly, it got the label of the image right. Loading the training data was less than 10 lines of code and the model itself is 1 line. It could handle random animal images and classify them regardless of the input image size, image format, or anything else.</p>

<h2 id="improving-our-model">Improving Our Model</h2>

<p>The one-line-of-code model is great, but we might want to tweak the model and compare the results to increase the accuracy. We will explore 4 techniques or tools that can improve the model:</p>

<ol>
  <li>Learning rate finder</li>
  <li>Transfer Learning</li>
  <li>Discriminative Learning rates</li>
  <li>Selecting the right number of epochs</li>
</ol>

<h2 id="learning-rate-finder">Learning rate finder</h2>

<p>The general idea of a <em>learning rate finder</em> is to start with a very very small learning rates, watch the loss function, and iterating with bigger and bigger learning rates.</p>

<p>We start with some number so small that we would never expect it to be too big to handle, like .00000007. We use that for one mini-batch, track the loss, and double the learning rate. We keep doing it until the loss gets worse. Once it started to get worse and worse, we should select a learning rate a bit lower than that point.</p>

<p>fastai method <code class="language-plaintext highlighter-rouge">lr_find()</code> does all this loop for us:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span> <span class="o">=</span> <span class="n">cnn_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet34</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="n">error_rate</span><span class="p">)</span>
<span class="n">learn</span><span class="p">.</span><span class="n">lr_find</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SuggestedLRs(valley=tensor(0.0025))
</code></pre></div></div>

<p><img src="/images/Fastbook/Fastbook_Chapter_5_Image_Classification_40_2.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Let's call it the "leslie_smith_lr" in honor to the author of the orignal paper
</span><span class="n">leslie_smith_lr</span> <span class="o">=</span> <span class="mf">0.0025</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">cnn_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet34</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="n">error_rate</span><span class="p">)</span>
<span class="n">learn</span><span class="p">.</span><span class="n">fine_tune</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">base_lr</span> <span class="o">=</span> <span class="n">leslie_smith_lr</span><span class="p">)</span>
</code></pre></div></div>

<table border="0" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.261431</td>
      <td>0.310061</td>
      <td>0.102842</td>
      <td>01:10</td>
    </tr>
  </tbody>
</table>

<table border="0" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.547525</td>
      <td>0.373586</td>
      <td>0.115020</td>
      <td>01:14</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.348811</td>
      <td>0.226233</td>
      <td>0.068336</td>
      <td>01:14</td>
    </tr>
  </tbody>
</table>

<p>Compared with the baseline model we reduced slightly the error_rate. In the next tables, I will keep track of the improvements and the comparison of the methods.</p>

<h2 id="transfer-learning-and-freezing">Transfer Learning and Freezing</h2>

<p><strong>Transfer learning</strong></p>

<p>The last layer in a CNN is the classification task. This pet breed classification task is a layer with 37 neurons with a softmax function that gives the probability of the image for each of the  37 classes. But how can we use all this hard-consuming weighting parametrization in another image classification task? We start all over? <strong>No!</strong></p>

<p>We can take the model, ditch the last layer and substitute it for our new classification task. For example, for a layer with 5 neurons that classify images into 5 different classes. That it‚Äôs the idea behind <strong>transfer learning</strong>, taking the parameters and model from one task and substituting the last layer for the new task without starting the weighting from scratch. It saves time and also produces better results. <code class="language-plaintext highlighter-rouge">restnet34</code> is an example of this, as it is a pre-trained model with its custom parametrization.</p>

<p><strong>Freezing</strong></p>

<p>Transfer learning can be applied by a technique called freezing. By freezing you tell the model not to touch certain layers. They are ‚Äúfrozen‚Äù.</p>

<p><em>Why you would want to freeze layers?</em></p>

<p>To focus on the layer that matters. As I said, <code class="language-plaintext highlighter-rouge">restnet34</code> is already trained beforehand. We can tell the model to focus more on the last layer, our classification task, and keep the former ones untouched. Freeze and unfreeze effectively allow you to decide which specific layers of your model you want to train at a given time.</p>

<p>Freezing is especially handy when you want to focus not only on the weighting but also on some parameters like the learning rate.</p>

<p>To allow transfer learning we can use <code class="language-plaintext highlighter-rouge">fit_one_cycle</code> method, instead of <code class="language-plaintext highlighter-rouge">fine_tune</code>. Here we load the model with our data and train it for 3 epochs:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span> <span class="o">=</span> <span class="n">cnn_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet34</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="n">error_rate</span><span class="p">)</span>
<span class="n">learn</span><span class="p">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">leslie_smith_lr</span><span class="p">)</span>
</code></pre></div></div>

<table border="0" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.220214</td>
      <td>0.328361</td>
      <td>0.103518</td>
      <td>01:13</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.559653</td>
      <td>0.242575</td>
      <td>0.080514</td>
      <td>01:11</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.340312</td>
      <td>0.220747</td>
      <td>0.069689</td>
      <td>01:11</td>
    </tr>
  </tbody>
</table>

<p>Consider this model parametrization ‚Äúfroze‚Äù. Using <code class="language-plaintext highlighter-rouge">unfreeze()</code> method allows the model to start over from the already weighting from <code class="language-plaintext highlighter-rouge">fit_one_cyle</code>, so it doesn‚Äôt start from random weighting but the ‚Äúfrozen‚Äù parameters from the 3 first epochs of <code class="language-plaintext highlighter-rouge">fit_one_cyle</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span><span class="p">.</span><span class="n">unfreeze</span><span class="p">()</span>
</code></pre></div></div>

<p>It is is easier for the model to start from a pre-trained weighting than with random weighting. To illustrate this point let‚Äôs try to search for an optimal learning rate again:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span><span class="p">.</span><span class="n">lr_find</span><span class="p">()</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SuggestedLRs(valley=4.365158383734524e-05)
</code></pre></div></div>

<p><img src="/images/Fastbook/Fastbook_Chapter_5_Image_Classification_49_2.png" alt="png" /></p>

<p>In this graph, the Loss axis is way smaller than the previous one. The model is already trained beforehand and therefore trying mini-batches of different learning rates and iterating is easier.</p>

<p>To apply ‚Äútransfer learning‚Äù we train the model with another 6 epochs that will start from the previous parametrization. We use the new learning rate as well.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">leslie_smith_lr_2</span> <span class="o">=</span> <span class="mf">4.365158383734524e-05</span>
<span class="n">learn</span><span class="p">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">leslie_smith_lr_2</span><span class="p">)</span>
</code></pre></div></div>

<p>Instead of printing the epoch results, from here on I‚Äôll show the results of the last epoch and the comparison with the other models:</p>

<table border="0" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th>Model</th>
      <th>Train Loss</th>
      <th>Validation Loss</th>
      <th>Error rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ResNet-34 Baseline (2 epochs)</td>
      <td>0.330700</td>
      <td>0.223524</td>
      <td>0.071042</td>
    </tr>
    <tr>
      <td>ResNet-34 with Learning rate finder (2 epochs)</td>
      <td>0.348811</td>
      <td>0.226233</td>
      <td>0.068336</td>
    </tr>
    <tr>
      <td><b>ResNet-34 with Transfer Learning (6 epochs)</b></td>
      <td><b>0.534172</b></td>
      <td><b>0.261891</b></td>
      <td><b>0.083897</b></td>
    </tr>
  </tbody>
</table>

<h2 id="discriminative-learning-rates">Discriminative learning rates</h2>

<p>Even after we unfreeze, we still care a lot about the quality of those pre-trained weights. We would not expect that the best learning rate for those pre-trained parameters would be as high as for the randomly added parameters, even after we have tuned those randomly added parameters for a few epochs</p>

<p>Like many good ideas in deep learning, the idea of <strong><em>Discriminative learning rates</em></strong> is extremely simple: use a lower learning rate for the early layers of the neural network,
and a higher learning rate for the later layers</p>

<p>The first layer learns very simple foundations, like image edges and gradient detectors; these are likely to be just as useful for nearly any task. The later layers learn much more complex concepts, like the concept of ‚Äúeye‚Äù and ‚Äúsunset,‚Äù which might not be useful in your task at all (maybe you‚Äôre classifying car models, for instance). So it makes sense to let the later layers fine-tune more quickly than earlier layers.</p>

<p>By default, fastai <code class="language-plaintext highlighter-rouge">cnn_learner</code> uses discriminative learning rates.</p>

<p>Let‚Äôs use this approach to replicate the previous training, but this time:</p>

<p>The first value passed will be the learning rate in the earliest layer of the neural network, and the second value will be the learning rate in the final layer. The layers in between will have learning rates that are multiplicatively equidistant throughout that range.</p>

<p>Let‚Äôs use this approach to replicate the previous training, but this time we‚Äôll set only the lowest layer of our network to a learning rate of 4e-6; the other layers will scale up to 4e-4.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Model
</span><span class="n">learn</span> <span class="o">=</span> <span class="n">cnn_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet34</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="n">error_rate</span><span class="p">)</span>
<span class="c1"># Pre-train the model
</span><span class="n">learn</span><span class="p">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">leslie_smith_lr</span><span class="p">)</span>
<span class="n">learn</span><span class="p">.</span><span class="n">unfreeze</span><span class="p">()</span>
<span class="c1"># Train the model with a learning rate range
</span><span class="n">learn</span><span class="p">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="n">lr_max</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="mf">4e-6</span><span class="p">,</span><span class="mf">4e-4</span><span class="p">))</span>
</code></pre></div></div>

<table border="0" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th>Model</th>
      <th>Train Loss</th>
      <th>Validation Loss</th>
      <th>Error rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ResNet-34 Baseline (2 epochs)</td>
      <td>0.330700</td>
      <td>0.223524</td>
      <td>0.071042</td>
    </tr>
    <tr>
      <td>ResNet-34 with Learning rate Finder (2 epochs)</td>
      <td>0.348811</td>
      <td>0.226233</td>
      <td>0.068336</td>
    </tr>
    <tr>
      <td>ResNet-34 with Transfer Learning (6 epochs)</td>
      <td>0.534172</td>
      <td>0.261891</td>
      <td>0.083897</td>
    </tr>
    <tr>
      <td><b>ResNet-34 with Discriminative learning rates (12 epochs)</b></td>
      <td><b>0.049675</b></td>
      <td><b>0.181254</b></td>
      <td><b>0.048714</b></td>
    </tr>
  </tbody>
</table>

<h2 id="selecting-the-number-of-epochs">Selecting the Number of Epochs</h2>

<p>The more epochs, the more time and tries the model has to learn the trained data. Your first approach to training should be to simply pick a specific number of epochs that you are happy to wait for, and look at the training and validation loss plots.</p>

<p>Using <code class="language-plaintext highlighter-rouge">.plot_loss()</code> you can see if the validation loss keeps getting better with more epochs. If not, it is a waste of time to use more than the necessary epochs.</p>

<p>For some machine learning problems is worth keep training the model for a day to earn 1% more accuracy, such as programming competitions, but in most cases choosing the right model or better parametrization is going to be more important than squishing the last marginal accuracy point with 300 more epochs.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span><span class="p">.</span><span class="n">recorder</span><span class="p">.</span><span class="n">plot_loss</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/images/Fastbook/Fastbook_Chapter_5_Image_Classification_60_0.png" alt="png" /></p>

<h2 id="deeper-architectures">Deeper Architectures</h2>

<p>In general, a model with more parameters can describe your data more accurately. A larger version of a ResNet will always be able to give us a better training loss, but it can suffer more from overfitting, basically because it has more parameters to suffer from overfitting.</p>

<p>Another downside of deeper architectures is that they take quite a bit longer to
train. One technique that can speed things up a lot is <strong>mixed-precision training</strong>. This
refers to using less-precise numbers (half-precision floating point, also called fp16)
where possible during training.</p>

<p>Instead of using <code class="language-plaintext highlighter-rouge">.fit_one_cycle()</code> and then <code class="language-plaintext highlighter-rouge">unfreeze()</code> methods we tell fastai how many epochs to freeze with <code class="language-plaintext highlighter-rouge">freeze_epochs</code> since we are not changing the learning rates from one step to the other like in Discriminative learning rates.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">fastai.callback.fp16</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">cnn_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet50</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">error_rate</span><span class="p">).</span><span class="n">to_fp16</span><span class="p">()</span>
<span class="n">learn</span><span class="p">.</span><span class="n">fine_tune</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">freeze_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<table border="0" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.260760</td>
      <td>0.327534</td>
      <td>0.095399</td>
      <td>01:07</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.595598</td>
      <td>0.297897</td>
      <td>0.089310</td>
      <td>01:07</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.431712</td>
      <td>0.256303</td>
      <td>0.089986</td>
      <td>01:07</td>
    </tr>
  </tbody>
</table>

<table border="0" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.286988</td>
      <td>0.246470</td>
      <td>0.079161</td>
      <td>01:09</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.323408</td>
      <td>0.258964</td>
      <td>0.091340</td>
      <td>01:08</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.262799</td>
      <td>0.315306</td>
      <td>0.083221</td>
      <td>01:09</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.167648</td>
      <td>0.242762</td>
      <td>0.073072</td>
      <td>01:09</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.090543</td>
      <td>0.180670</td>
      <td>0.056834</td>
      <td>01:09</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.060775</td>
      <td>0.174947</td>
      <td>0.050068</td>
      <td>01:09</td>
    </tr>
  </tbody>
</table>

<h2 id="final-model-results-comparison">Final model results comparison</h2>

<p>Based on the validation loss and the error rate, a deeper and more complex architecture(RestNet50) and the model with discriminative learning rates hold the best results.</p>

<table border="0" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th>Model</th>
      <th>Train Loss</th>
      <th>Validation Loss</th>
      <th>Error rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ResNet-34 Baseline (2 epochs)</td>
      <td>0.330700</td>
      <td>0.223524</td>
      <td>0.071042</td>
    </tr>
    <tr>
      <td>ResNet-34 with Learning rate Finder (2 epochs)</td>
      <td>0.348811</td>
      <td>0.226233</td>
      <td>0.068336</td>
    </tr>
    <tr>
      <td>ResNet-34 with Transfer Learning (6 epochs)</td>
      <td>0.534172</td>
      <td>0.261891</td>
      <td>0.083897</td>
    </tr>
    <tr>
      <td>ResNet-34 with Discriminative learning rates (12 epochs)</td>
      <td>0.049675</td>
      <td>0.181254</td>
      <td><b>0.048714</b></td>
    </tr>
    <tr>
      <td>Mixed-Precision ResNet-50 (6 epochs)</td>
      <td>0.060775</td>
      <td><b>0.174947</b></td>
      <td>0.050068</td>
    </tr>
  </tbody>
</table>

<p>In any case, these techniques should be tried and evaluated for every image classification problem, as the results depend on the specific data. This is just an example of the applications and could easily improve any initial model baseline.</p>
:ET