I"t<<h1 id="mit-6s191---lecture-4---deep-generative-modeling">MIT 6.S191 - Lecture 4 - Deep Generative Modeling</h1>

<h2 id="generative-modeling">Generative modeling</h2>

<p>Deep Generative Modeling is part of <strong>unsupervised learning: the models receive the data but not the respective labels</strong>. The goal is to take as input the training samples from some distribution and learn a model that represents that distribution.</p>

<p>Another way to define this goal is to <strong>find ways to learn the underlying and hidden latent variables</strong> in the data even when the generative model is only given the representation of the variables.</p>

<p><img src="/images/MIT_deep_learning_intro/L4_images_generated.png" alt="" /></p>

<p>Deep generative models are very useful to create synthetic samples using the probability density function of the samples provided.</p>

<h2 id="use-examples">Use examples</h2>

<ul>
  <li>Debiasing image recognition</li>
</ul>

<p>Let’s say that you have a silly algorithm that takes facial expressions and the goal is classifying between <em>pretty</em> or <em>non pretty</em>. However, all your faces are either <em>white-blond-people smiling at the camera</em> or portraits of <em>drug addicts</em>. This algorithm won’t create a boundary between pretty and not, it would define a boundary between white-blond-smiling people and drug users. Generative models can follow the facial distribution of the existing sample to create new samples of portraits with different skin tones, postures and atributes.</p>

<ul>
  <li>Outlier detection in images</li>
</ul>

<p>Rare events in tail distributions, such as people crossing the street in red, accidents, or sudden impacts can be created by generative models as samples to train the model of self-driving cars. The benefit is that the car would know what to do in these extreme scenarios even if it hasn’t seen it before in the sample.</p>

<h2 id="autoencoding">Autoencoding</h2>

<p>Autoencoding means <strong>auto</strong>matically <strong>enconding</strong> data. In Generative Modeling, the <em>Encoder</em> learns to map from the data \(x\) into a low-dimensional vector \(z\):</p>

<p><img src="/images/MIT_deep_learning_intro/L4_autoencoder.png" alt="" title="Autoencoders: background" /></p>

<p><strong>Autoencoding is a form of compression</strong>. A smaller dimensionality of the latent space means that we can compress the data into smaller latent factors that keep the feature representation.</p>

<p>However, the dimensionality of the latent space will also influence the reconstruction quality. The smaller the latent space the poorer and less quality the generated images have, as will force a larger training to bottleneck.</p>

<p>But wait, the input data has no labeled. Thefore, \(z\) cannot be a <em>feature map</em> of the atributes of 2s, as this algorithm doesn’t know is a 2 in the first place!</p>

<p><strong>What is this \(z\) then?</strong></p>

<p><strong>\(z\) is vector of latent variables</strong>. It represent the features of the image in a lower dimensional vector space, in this case the features of a 2.</p>

<p>The model uses the features created in this latent space \(z\) to construct a new observations \(\hat{x}\) following the features of the original \(x\). It “decodes” the original images to create new images.</p>

<p><strong>How the algorithm knows that the atributes in \(z\) are right?</strong></p>

<p>The model learns by comparing the difference between the new synthetic image and the original image in terms of pixels.</p>

<p><img src="/images/MIT_deep_learning_intro/L4_autoencoder2.png" alt="" title="Autoencoders: mapping the latent space" /></p>

<p>Therefore, can be trained to minimize the Mean Squared Error between the sample inputs \(x\) and ouput synthetic samples \(\hat{x}\).</p>

<h2 id="variational-autoencoders-vaes">Variational Autoencoders (VAEs)</h2>

<p>In the previous image, the latent space \(z\) acts as a “normal” layer in a Neural Network. Is deterministic in the sense that it would yield the same latent variable space \(z\) every time we use the same image as input.</p>

<p>In contrast, <strong>VAEs impose a variational or stochastic</strong> twist to the architecture to generate smoother and different representations of the images:</p>

<p><img src="/images/MIT_deep_learning_intro/L4_autoencoder3.png" alt="" title="Variational Autoencoders" /></p>

<p>For each variable, the VAE learns a mean and a variance associated with that latent variable. Instead of using the vector latent variables \(z\) straight, the model uses the vector of means and a vector of variances to define the probability distributions for each of the latent variables.</p>

<p>The goal of this twist is to generate slightly different new images from the samples, not to imitate perfectly them perfectly.</p>

<h2 id="vae-operations">VAE Operations</h2>

<p>VAEs optimization process can be dividing into: <strong>encoding and decoding</strong>.</p>

<p><img src="/images/MIT_deep_learning_intro/L4_autoencoder4.png" alt="" title="Variational Autoencoders Optimization" /></p>

<ol>
  <li><strong>Encoding</strong>.</li>
</ol>

<p>The first part of the process is called <em>encoding</em>, as it encode or define the latent space \(z\) given \(x\) observations.</p>

<p>Learning the structure of the input images by deconstruction, comparing the differences between the distribution of features input images and new images (log-likelihood). Optimizing the \(q_{\phi}\) weights.</p>

<ol>
  <li><strong>Decoding</strong>.</li>
</ol>

<p>The second part of the process is called <em>decoding</em>, as it decodes or extract the features of the latent space \(z\) to make new observations \(\hat{x}\).</p>

<h2 id="the-vae-regularization">The VAE regularization</h2>

<p>The training phase will change as a result of these two different tasks. The loss function cannot be only calculated as the difference in similarity between input and output images, as they must be different by definition. This is the stochastic <em>twist</em> necessary to create new images, not just copies.</p>

<p>The optimization function must include a new term, the <strong>VAE loss:</strong></p>

<p><strong><center>VAE Loss function = (reconstruction loss) + (regularization term)</center></strong></p>

<p>As in any other neural network, the regularization term avoids overfitting. In this neural network architecture overfitting would mean replicating the same exact images of \(x\) into \(\hat{x}\). We don’t want the same images, we want different images that follows the latent varaibles of the original sample.</p>

<p><img src="/images/MIT_deep_learning_intro/L4_autoencoder5.png" alt="" title="Variational Autoencoders Optimization: Loss function" /></p>

<p>By adding this new parameter \(D\) to the loss function, the Neural Network will try to reduce not only the errors extracting the latent variables (reconstruction loss) but also avoid overfitting the model so it doesn’t create identical copies of the input images (regularization term).</p>

<p>Let’s analyze this regularization term analytically: \(D\left(q_{\phi}(\mathrm{z} \mid x) \| p(z)\right)\)</p>

<p>\(D\) is a function of:</p>

<ul>
  <li>
    <p>\(q_{\phi}(z \mid x)\): the encoding. Imposes to the new synthetic images \(\hat{x}\) to follow a inferred latent distribution of the latent variables \(z\).</p>
  </li>
  <li>
    <p>\(p(z)\): the decoding. Imposes to the new synthetic images \(\hat{x}\) to follow a prior <strong>fixed prior</strong> distribution of \(z\)</p>
  </li>
  <li>
    <p>Finally, the two vertical lines between the elements of the function is a reciprocal math operator. Effectively, it means that \(D\) is function of the difference between the two elements, the <strong>inferrerd</strong> and the <strong>fixed prior</strong> distribution.</p>
  </li>
</ul>

<p>In other words, \(D\) is a parameter that represents the divergence of what the encoder is trying to infer and a prior distribution of \(z\).</p>

<p>The <strong>inferred</strong> distribution of \(z\) is easy to understand, as it is just the latent variables of the images created by using the mean and standard deviation of each input.</p>

<p>However…<strong>What is a fixed prior distribution? How it calculates the \(p(z)\) ?</strong></p>

<h2 id="priors-on-the-latent-distribution">Priors on the latent distribution</h2>

<p>The usual prior distribution choice is the <strong>normal Gaussian distribution</strong> (means equal 0 and standard deviations equals 1). In practical terms, this prior makes the model cut the features that are way out of a normal distribution, such as outliers or edge cases in the data.</p>

<p>The new samples generated \(\hat{x}\) follows the inferred distribution but also this fixed prior. The loss funtion optimize the inferred latent distribution and also penalize extreme cases outside the normal distribution (<em>weird or non-common elements in the images</em>)</p>

<p><img src="/images/MIT_deep_learning_intro/L4_normaldis.png" alt="" title="Normal Gaussian distribution used to setting prior distribution" /></p>

<p>We said that the regularization term <em>D</em> is a function of the difference between the inferred latent distribution and a Gaussian prior distribution. This difference is called <strong>KL-divergence</strong>(Kullback-Leibler) or <strong>relative entropy</strong>.</p>

\[D\left(q_{\phi}(\mathrm{z} \mid x) \| p(z)\right) =-\frac{1}{2} \sum_{j=0}^{k-1}\left(\sigma_{j}+\mu_{j}^{2}-1-\log \sigma_{j}\right)\]

<p>While the form of the function looks <em>unfriendly</em>, it is just a measure of how one probability distribution is different from a second reference probability distribution.</p>

<h2 id="why-vae-regularization-is-important">Why VAE regularization is important?</h2>

<p>The VAE regularization  creates:</p>

<ol>
  <li><strong>Continuity</strong>. Data points that are similar in the latent space should result in similar content after the decoding.</li>
  <li><strong>Completeness</strong>. New samples out of the latent space should resemble meaningful content after decoding.</li>
</ol>

<p>Without regularization (a loss function that just tries to minimize the encoding loss), the model could group images that are similar in real life in different clusters because of the small variations. We want input images with close latent features to have very similar distributions that the model can use to create new ones.</p>

<p><img src="/images/MIT_deep_learning_intro/L4_regularization.png" alt="" title="Not regularized vs regularized data points in latent and output space" /></p>

<p>The normal prior force the latent learned distribution to overlap. For example, if we want to create faces with VAEs the fixed distribution forces images of faces to place the eyes, mouth, and ears within the same regions.</p>

<p><img src="/images/MIT_deep_learning_intro/L4_regularization2.png" alt="" title="Regularized data points in latent and output space clustered in close distributions" /></p>

<h2 id="vae-backpropagation-re-parametrization">VAE Backpropagation: Re-parametrization</h2>

<p>Backpropagation in Neural Networks requires deterministic nodes and layers (constant weights). Weights need to remain constant to calculate the chain rule to optimize the loss by gradient descent.</p>

<p>But remember that <strong>VAs impose a variational or stochastic</strong> twist in the forward propagation to generate new images and therefore you cannot backpropagate a sampling layer.</p>

<p><img src="/images/MIT_deep_learning_intro/L4_backpropagation.png" alt="" title="Backpropagation on $$z$$ original form" /></p>

<p>Well, you actually can. The hidden latent space \(z\) variation is not stochastic itself, it includes a random constant \(\varepsilon\). Therefore, \(z\) can be <em>reparametrized</em>, as \(\varepsilon\) is just a constant that follows a normal distribution.</p>

<p><img src="/images/MIT_deep_learning_intro/L4_backpropagation2.png" alt="" title="Backpropagation on $$z$$ reparametrized form" /></p>

<p>Notice that \(z\) goes from being a stochastic node (left) to being a deterministic one (rigth). Again, this is because \(z\) can be derived taking \(\varepsilon\) as a random constant that follows a normal distribution. The function loss can be minimized since the chain rule can be applied to optimize the weigths of the encoding loss \(q_{\phi}\).</p>

<h2 id="generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)</h2>

<p>GANs is another architecture to generate new data following the same distribution of the input data. The <em>Adversarial</em> part comes because in <strong>this architecture two neural networks contesting with each other in a zero-sum game where one network gain is the other network loss</strong>.</p>

<ol>
  <li><strong>Generator Network</strong></li>
</ol>

<p>This network is trained to get random noise data and <strong>produce new (fake) samples that represent the noise distribution</strong> as much as possible. The random noise can be created sample out of a Gaussian distribution.</p>

<p>There are no encoding loss, the new features extracted comes from noise.  Therefore, the distribution that the model is trying to learn comes from a random sampling, not real images.</p>

<p>Here in the next image, the Generator Network \(G\) learns from a normal Gaussian distribution \(z\) and creates new samples \(X_{fake}\) that follows this distribution.</p>

<p><img src="/images/MIT_deep_learning_intro/L4_generator.png" alt="" title="Generator Network producing a feature latent space out of noise" /></p>

<ol>
  <li><strong>Discriminator Network</strong></li>
</ol>

<p>This network takes the fake features from the Generator Network and real features from real images data. With both inputs, <strong>the Discriminator task is to identify the fake features from the real ones</strong>.</p>

<p><img src="/images/MIT_deep_learning_intro/L4_gan.png" alt="" title="Generative Adversarial Network structure" /></p>

<p><em>G</em> tries to synthesize fake instances that fool <em>D</em>, and <em>D</em> tries to identify these from real ones.</p>

<p>The two networks interact with each other, <strong>the better the Generator Network gets the hardest is for the Discriminator to tell apart</strong> fake from real features.</p>

<h2 id="gans-loss">GANs Loss</h2>

<p>As they have different goals, the Generator and Discriminator network have different loss funtions that combines into a <em>total</em> GAN loss funtion.</p>

<ol>
  <li>Discriminator Network Loss</li>
</ol>

\[\arg \max_{D} \mathbb{E}_{\mathbf{z}, \mathbf{x}}[\log D(G(\mathbf{z}))+\log (1-D(\mathbf{x}))]\]

<p>It maximizes the probability of the fake data to be identified as fake: $\log D(G(\mathbf{z}))$, and the real data being identified as real: $\log (1-D(\mathbf{x})$.</p>

<ol>
  <li>Generator Netowork Loss</li>
</ol>

\[\arg \min_{G} \mathbb{E}_{\mathbf{z}, \mathbf{x}}[\log D(G(\mathbf{z}))+\log (1-D(\mathbf{x}))]\]

<p>It minimizes the probability of the Discriminator Network <em>D</em> to identify fake data as fake: $\log D(G(\mathbf{z})$, and the real data being identified as real: $\log (1-D(\mathbf{x}))$.</p>

<p>We can combine both loss functions as the GANs Loss function:</p>

\[\arg \min_{G} \max_{D} \mathbb{E}_{\mathbf{z}, \mathbf{x}}[\log D(G(\mathbf{z}))+\log (1-D(\mathbf{x}))]\]
:ET