---
layout: course
title: MIT 6.S191 - Lecture 2 - Recurrent Neural Networks
---

{{ page.title }}
================

## Intro

From a single perceptron, we can extend the number of inputs, neurons and hield multi-dimensional outputs:

![](/images/MIT_deep_learning_intro/L2_multi_output.png)


But this multi perceptron, or Neural Network, doesn't have a sense of time or element sequence. Every input and output is a specific time step.

![](/images/MIT_deep_learning_intro/L2_sequence.png)


This lack of connection between time steps is problematic in predicting problems that involves time or sequences. In a sequence, the inputs are correlated with each other. They are not independent. For example, future sales in a given shop are correlated with previuos sales, they are not independent events.

Expresing it in the above graph, the output of $$\hat{y}_2$$ not only depends on $$X_2$$, but also on $$X_0$$ and $$X_1$$.

## The missing piece, the Cell state

To make use of the correlation of the inputs in sequence, the network would need to have a connection that allows to look forward. This connection is called internal memory or **cell state** $$h_t$$:

![](/images/MIT_deep_learning_intro/L2_multi_output.png)

**The memory or cell state pass the current information in the step $$t$$ to the next step $$t+1$$**.


## Recurrent Neural Networks 

Recurrent Neural Networks are the result of incorporating the idea of using cell states to pass throw information between time steps. **They can be thought of as multiple copies of the same network, each passing the new cell state value to a successor network**. Every network is a time step of the *global* neural network.

RNNs have a state $$h_t$$, that is updated at each time step as a sequence is processed. The recurrent relation applied at each and every time step is defined as:

![](/images/MIT_deep_learning_intro/L2_rec_rel.png)


The function is going to be parametrized by a set of weights that is leaned throughout training the model. **The same function and the very same parameters are applied every step of processing the sequence (every iteration of the model)**.

![](/images/MIT_deep_learning_intro/L2_rnn_ac.png)

- $$W_{xh}$$ denotes the weight matrix optimized for that specific step of the sequence.

- $$W_{hh}$$ denotes the weight matrix of the memory cell, reused every step for the entire sequence.

- $$W_{hy}$$ denotes the weight matrix of a combination of both the specific optimization of the weights for that step, and the memory cell matrix.

In practice, you won't see the cell states weigthing the outputs of the next step outputs, or multiple networks one after the other. The loop is made inside one single architecture. The RNN algorithm can be simplyfied as:

![](/images/MIT_deep_learning_intro/L2_rnn_eq.png)



## Examples of RNN application 

Recurrent Neural Networks are usually used in text problems such as sentiment classification, text generation from an image, generation of image title or translation.

![](/images/MIT_deep_learning_intro/L2_cat_words.png)


This is an example using **many** words **to predict the one** next word in the sentence. Depending on the problem, the number of inputs and outputs change, that modify the NN architecture:

![](/images/MIT_deep_learning_intro/L2_examples_rnn.png)


## Making Neural Networks understand text: Embedding 

Neural Networks do not understand word language, or images, they only understand numbers. They require the words to be parsed as vectors or arrays of numbers:

![](/images/MIT_deep_learning_intro/L2_words.png)

**How are this vectors made?**

1. The computer/algorithm gets all the words and create a **vocabulary** with them.

2. Then, it creates its own dictionary to understand them, assigning a number to each different word (**indexing**).

3. The numbers form vectors of a fixed size that captures the content of the word (**embedding**).

By using vectors and not single numbers, you can compare how close are vectors to each other. And comparing distance is key because the words that usually go together in a phase must be represented by vectors close to each other. For example, the vector of *dog* is closer to the vector of *cat* than to the vector of *sad*.

**Embedding gather words together by similarity using the distance between vectors.**

![](/images/MIT_deep_learning_intro/L2_embedding.png)

## Model Design Criteria, or why RNN are good 

Any recurrent model architecture must the following design criteria:

1. Must handle variable-length sequences (RNN ✔️)

![](/images/MIT_deep_learning_intro/L2_length.png)

2. Must track long-term dependencies (RNN ✔️)

![](/images/MIT_deep_learning_intro/L2_long_dep.png)

3. Must mantain information about order (RNN ✔️)

![](/images/MIT_deep_learning_intro/L2_order.png)

4. Must share parameters across the sequence (RNN ✔️)

In RNNs the same memory cell is reused every step for the entire sequence, as explained previusly.


## RNN Ilustrated example (from Michael Phi) 

Let's say that we want to do a many-to-one prediction in which the inputs are words in this cereal review and the output is a positive or negative sentiment analysis.

![](https://miro.medium.com/max/1400/1*YHjfAgozQaghcsEvsBEu2g.png)

First the words are transformed to vectors by embedding.

From:

![](/images/MIT_deep_learning_intro/L2_LSTM_1.png)

To:

![](https://miro.medium.com/max/1400/1*AQ52bwW55GsJt6HTxPDuMA.gif)

While processing, it passes the previous hidden state to the next step of the sequence. The hidden state acts as the neural networks memory. It holds information on previous data the network has seen before.

![](https://miro.medium.com/max/1400/1*o-Cq5U8-tfa1_ve2Pf3nfg.gif)

For every of theses steps or layers, the input and previous hidden state are combined to form a vector. It goes through a tanh activation, and the output is the new hidden state $$h_t$$. The tanh function ensures that the values stay between -1 and 1.

![](https://miro.medium.com/max/1400/1*WMnFSJHzOloFlJHU6fVN-g.gif)


## Backpropagation Through Time (BPTT) 

The usual NN backpropagation algorithm:

1. Take the derivative (gradient) of the loss with respect to each parameter $$W$$.
2. Shift parameters to minimize loss.

With a basic Neural Network, the backpropagation errors goes trough a single feedforward network for a single time step.

Recurrent Network backpropagation needs a twist, as it contains multiple steps and a memory cell. In RNNs, **the errors are backpropagating from the overall loss through each time step**:

![](/images/MIT_deep_learning_intro/L2_BPTT.png)

The key difference is that the gradients for $$W$$ at each time step are summed. A traditional NN doesn't share parameters across layers. Every input is different and have different weigths $$W$$.

## Problems with backpropagation in RNN 

Computing the gradient with respect to the initial $$h_0$$ involves many matrix multiplications between the memory cell $$h_t$$ and the weights $$W_hh$$.

### Exploiting gradients (gradients > 1.0)

In the the process of backpropagation the gradients get multiplied by each other over and over again. If they are larger than 1.0, the end matrix of weigths is huge.

As a silly example: 0.5 times 1.5 is 0.75, 0.5 times 1.5^200 is 8.2645996e34. This can give you a perspective of how matrix multiplication can explote by mutliplying constantly by 1.X.

These huge gradients can become extremely large as the result of matrix and the loss function cannot be minimized.

The usual solution is change the derivative of the errors before they propagate through the network, so they don't become huge. Basically, you can create a threshold that the gradients cannot surpass. *Create a threshold* means that you set a value, such as 1.0, that forces the values to be 1.0 at maximum.


### Avoid exploiting gradients: Gradient thresholds

There are two ways to create these thresholds:

**1. Gradient Norm Scaling**

Gradient norm scaling rescales the matrix so the gradient equals 1.0 if the a gradient exceeds 1.0.

Gradient Norm Scaling in Tensorflow:

```python
  opt = SGD(lr=0.01, clipnorm=1.0)
```

**2. Gradient Value Clipping**

Gradient value clipping simply forces all the values above the threshold to be the threshold, without changing the matrix. If the clip is 0.5, all the gradient values less than -0.5 are set to -0.5 and all the gradients more than 0.5 set to 0.5.

Gradient Norm Scaling in Tensorflow:

```python
  opt = SGD(lr=0.01, clipvalue=0.5)
```


### Vanishing gradients (gradients < 1)

As gradients can become huge they can also become tiny to the point that it is not possible to effectively train the network.

This is a problem because the errors further back in time are not being propagated. It would cause that the long-term errors are vanished and bias the model only to capture short-term dependencies.

### Avoid vanishing gradients

The basic recipe to solve vanishing gradients is use a ReLU activation function, chaning to a smart weight initialization and/or use a different RNN architecture.

**1. Change activation function to ReLU.**

Why ReLu?

Because when the cell or instance gets activated (weight 0 or more), by definition the derivative or gradient is 1.0 or more:

![](/images/MIT_deep_learning_intro/L2_activation_trick.png)

**2. Change weight initialization.**

For example to the **Xavier initialization/Glorot initialization**:

Changing the weight activation in Tensorflow:

```python
from keras.models import Sequential
from keras.layers import Dense, Activation

model = Sequential([
    Dense(16, input_shape=(1,5), activation='relu'),
    Dense(32, activation='relu', kernel_initializer='glorot_uniform'),
    Dense(2, activation='softmax')
])
```
**3. Change Network architecture.**

More complex RNNs such as **LSTM or GRU** can control the information that is passing through. Long Short Term Memory networks (**LSTM**) and Gated Recurrent Units (**GRU**) are special kinds of RNN, capable of learning long-term dependencies.

![](/images/MIT_deep_learning_intro/L2_activation_trick3.png)

They can keep informed of long-term dependencies **using filters or gates**. In essence, these gates decide how much information to keep of the previous neuron state or values, and how much to drop. This makes the optimization problem or the Neural Network less prompt to vanishing or exploding gradient problems.

## LSTM 

In a simple RNN, the information goes though every step with the input of that time step ($$x_t$$), the previous step memory cell ($$h_{t-1}$$) and an output for every step ($$y_t$$).

![](/images/MIT_deep_learning_intro/L2_rnn_arq.png)

The structure of a LSTM is more complex. **LSTM forces the matrix inputs in every step to go through gates**, or internal mechanism to keep long-term information.

![](/images/MIT_deep_learning_intro/L2_lstm_arq.png)

## LSTM Gates system

They 4 types of gates interacting within each step layer:

1. ***Forget gate***: Remove the irrelevant information.

Information from the previous hidden state and the current input is passed through the sigmoid function. Values come out between 0 and 1.

The closer to 0 means to forget, and the closer to 1 means to keep.

![](https://miro.medium.com/max/1400/1*GjehOa513_BgpDDP6Vkw2Q.gif)

2. ***Store gate***: Store relevant information.

The same previous $$h_{t-1}$$ and the current inputs goes into two transformations:

- Sigmoid transformation. It is the same operation as before, but in another gate. Instead of forget and keep, it will decide the information to update or not update.

- Than transformation. It helps to regulate the network by squishing values between -1.0 and 1.0.

The matrix multiplication of the tanh outputs with the sigmoid outputs decides which information is important, and store it in a cell state $$\bigotimes$$.

![](https://miro.medium.com/max/1400/1*TTmYy7Sy8uUXxUXfzmoKbA.gif)

3. ***Update gate***: update the separated cell state.

- The update gate takes the previous cell state vector $$c_{t-1}$$ and multiply by the forget vector (from the forget gate), that allows to drop non-important information.

- Then, it adds the store vector from the store gate, as this information is important to keep from the current step.

![](https://miro.medium.com/max/1400/1*S0rXIeO_VoUVOyrYHckUWg.gif)

The update gate takes the information to the other 2 gates to decide what to forget and what to keep, updating the cell state.

4. ***Output gate***: decides what the next hidden state $$h_{t+1}$$.

- The previous hidden state and the current input into a sigmoid function.
- Then the newly modified cell state pass the tanh function.
- By multiplying the two vectors it decides what information the hidden state should carry.

![](https://miro.medium.com/max/1400/1*VOXRGhOShoWWks6ouoDN3Q.gif)

## GRU

GRU’s has fewer tensor operations; therefore, they are a little speedier to train then LSTM’s. There isn’t a clear winner which one is better, try both to determine which one works better for their use case.

![](https://miro.medium.com/max/1400/1*jhi5uOm9PvZfmxvfaCektw.png)